group_by_length: True
logging_steps: 10
do_train: True
do_eval: True
do_predict: True
seed: 42

# https://github.com/pytorch/fairseq/blob/7f5ec30/examples/wav2vec/xlsr/config/finetune.yaml#L10-L14
evaluation_strategy: steps
save_strategy: steps
save_steps: 1000
eval_steps: 10
metric_for_best_model: wer
greater_is_better: False
save_total_limit: 2

# https://github.com/pytorch/fairseq/blob/7f5ec30/examples/wav2vec/xlsr/config/finetune.yaml#L4
fp16: True
fp16_full_eval: True

# https://github.com/pytorch/fairseq/tree/7f5ec30/examples/wav2vec/xlsr
per_device_train_batch_size: 8  # Need 3 GPUs for batch_size=24
per_device_eval_batch_size: 16
gradient_accumulation_steps: 1

# Inside the paper:
# We determine the best learning rates setting in [2e-5, 6e-5] based on dev set error rate
learning_rate: 6e-5

# https://github.com/pytorch/fairseq/blob/7f5ec30/examples/wav2vec/xlsr/config/finetune.yaml#L44-L47
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08

# Inside the paper:
# warm up for the first 10% of updates,
# keep constant for 40% and then linearly decay for the remainder.
# For CommonVoice, we fine-tune for 20k updates.
# Most similar implementation w/o major code fixes: cosine + warmup
max_steps: 20000
warmup_steps: 2000
lr_scheduler_type: cosine
